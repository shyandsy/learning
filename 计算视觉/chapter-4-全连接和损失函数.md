### 全连接神经网络（多层感知器）和损失函数

-----------------------------

- [x] 上节回顾
- [x] 全连接神经网络基本概念
- [x] 分类模型
  - [x] 多层感知器
  - [x] 激活函数
- [x] 损失函数
  - [x] softmax与交叉熵
  - [x] 对比多累支撑向量机损失
- [ ]  优化算法
  - [x] 计算图与反向传播<font color="red">我他妈没看懂！！！</font>
  - [ ] 再谈激活函数
  - [ ] 动量法与自适应梯度



![alt](images\全连接神经网络-overview.png)



#### 上节回顾：线性分类器

$f(x, W) = Wx + b$

其中，x代表输入图像，维度是d

 $w = [w_1, w_2....w_c]^T$ 	为权值矩阵

​	 $w_i = [w_{i1}, w_{i2}....2_{id}]$，为第i类别的权值向量

$b = [b_1,b_2,...b_c]^T$，偏值向量，$b_i$为第i个类别的偏值



#### 全连接神经网络

全连接神经网络级联多个线性变换来实现输入到输出的映射



<font color="red">两层全连接神经网络</font>

$f = W_2max(0, W_{1}x + b) + b_2$

首先经过第一个线性变换处理$X_1 = W_{1}x + b$，在经过外层第二个线性变换$f = W_2max(0, X_1) + b_2$



<font color="red">三层全连接神经网络</font>

$f = W_3 max(0, W_2max(0, W_{1}x + b) + b_2)$



非线性操作max，不可以去掉。去掉就不是全连接神经网络，而是退化为线性分类器



#### 全连接神经网络的权值

线性分类器中，权值矩阵W的行数是固定的，和类别数相等。 权值矩阵W就可以看作是模板。



<font color="red">而全连接神经网络中，只对外层W有要求，内层可以自定定义行数</font>

$f = W_2max(0, W_{1}x + b) + b_2$， max对结果矩阵逐个执行max操作

$W_2$行数必须和类别数相同，$W_1$没有要求

- $W_1$可以被看作模板，模板个数认为指定
- $W_2$融合着多个末班的匹配结果来实现最终类别打分



那我们指定W2的行数有什么用？？

下面这张图是上一节中线性分类器的模板。

![alt](images\全连接神经网络的模板.png)

注意看这个马，他有2个头！！！真实的马是不可能有2个头的。这说明一个类别对应一个特征向量（1个模板图）是不够的的。

如果我们马的模板（行数）够多（比如100个），1个马记录头朝左边，1个马记录头朝右边，那只要我有足够多的马模板，我就可以让模型更加精确。

假设我们指定$W_2$行数100，用5行来存储5个马的模板，那么我们可以用$W_2$融合多个末班的匹配结果来实现最终的类别打分

|       |             |       |       |
| ----- | ----------- | ----- | ----- |
| 马1   | 马模板向量1 |       |       |
| 马2   | 马模板向量2 |       |       |
| 马3   | 马模板向量3 |       |       |
| 猫1   | 猫模板向量1 |       |       |
| ..... | .....       | ..... | ..... |



##### $W_1$维度分析

行数：模板个数，假设我们有100个模板，就100行

列数：输入图片的维度，比如3072

因此

​	权值矩阵$W_1$ 维度：100 * 3072

​	$b_1$维度: 100 * 1



##### $W_2$维度分析

有上面分析可知，$max(0, W_{1}x + b)$结果是100 * 1

$W_2$: 10个类别，所以只能是10行。内部$w_1$是100 * 1，因此$w_2$只有100列。因为

​	$W_2$是 10 *100 矩阵

$b_2$只能是： 100 * 1



#### 全连接神经网络与线性不可分

两层全连接网络

$f = W_2max(0, W_{1}x + b) + b_2$



线性分类器，通常解决线性可分的任务。

![alt](images\线性可分.png)

但并非所有情况都可以找到线性分隔，比如

![alt](images\线性不可分.png)

此时我们就需要使用神经网络来决绝这类问题，我们需要全连接神经网络（最简单的非线性分类器），当然除了全连接神经网络我们还有很多其他的非线性分类器。

![alt](images\线性不可分2.png)



-------------------------------------------------------------------------------



#### 全连接神经网络绘制与命名

两层全连接网络

$f = W_2max(0, W_{1}x + b) + b_2$

三层全连接神经网络

$f = W_3 max(0, W_2max(0, W_{1}x + b) + b_2)$

我们需要多少层，就可以级联多少层



全连接神经网络的两种表达方式

表达1

![alt](images\全连接神经网络-表达1.png)

表达2（常用）

![alt](images\全连接神经网络-表达2.png)

输入层，我们例子中3072维（图片向量）

隐层：模板个数，比如上面例子中的100

输出层：固定，类别数决定



隐层的每一个神经元，要和前层的所有神经元相连，所以叫全连接神经网络。



<font color="red">N层全连接神经网络</font>： 除输入层之外，其他层的数量为N的网络

<font color="red">N个隐层的全连接神经网络</font>：网络隐层的数量为N的网络

![alt](images\三层全连接神经网络.png)



#### 激活函数

max操作：对向量中每一个维度进行比较，最终得出一个结果

例子

max([0,0,0,0], [-2, -1, 3, 1]) = [0, 0, 3, 1] 高通滤波器



max就是一种激活函数



<font color="red">如果全连接神经网路中缺少了激活函数，它就会退化为线性分类器。</font>



除了max，还有很多常用的激活函数。

常用的激活函数有四种

- sigmoid：会让结果变为（0，1）之间。当x大于5，输出趋于1；当x趋于-5，输出趋于0

  ![alt](images\激活函数-sigmoid.png)

- tanh：会把结果压缩到(-1,1)

  ![alt](images\激活函数tanh.png)

- ReLU：会把结果映射到$(0, +\infin)$

  ![alt](images\激活函数-ReLU.png)

- Leaky ReLU：会把结果压缩到$(0.1x, x)$

  ![alt](images\激活函数leaky-relu.png)

#### 网络结构设计

结构设计的关键问题

1. 用不用隐层，用一个函数用几个隐层？（深度设计）
2. 每个隐层设置多少个神经元比较合适？（宽度设计）

<font color="red">没有标准答案。。。</font>



1. 神经元个数越多，分界面就可以越复杂，在这个集合上非线性分类能力就越强。

![alt](images\神经元个数越多非线性分类能力越强.png)

2. 增加神经网络的层数，也可以提升网络的非线性分类能力



- 根据分类任务的难易程度来调整神经网络模型的复杂程度。

- 分类任务越难，我们设计的神经网络结构就应该越深，越宽

- 但是，对训练集分类精度最高的全连接神经网络模型，在真实场景下识别性能未必最好（过拟合）



#### 全连接神经网络小结

- 全连接神经网络组成：1个输入层，1个输出层，多个隐层
- 输入层与输出层的神经元个数由任务决定，隐层数量以及每个隐层的神经元个数需要认为指定
- 激活函数是全连接神经网络中一个很重要的部分，缺少了激活函数，全连接神经元将退化为线性分类器



----------------------------------------------------------



#### 损失函数

##### softmax与交叉熵

两层全链接神经网络

$f = W_2max(0, W_{1}x + b) + b_2$



softmax操作

![alt](images\softmax.png)

softmax计算过程：先取指数次方，在做归一化

对于计算分数结果f1：

​	首先，指数：$f1^` = `e^{f_i}$

​	在进行归一化：$\frac{t_i}{\sum_{j} t_j }$     每一项$t_i$除以$sum(t_i)$

最后得出各种标签的概率

![alt](images\softmax计算.png)



##### 交叉熵损失

上面预测概率 vs 真实分布

$\begin{bmatrix} 0.21 \\ 0.01 \\ 0.78 \end{bmatrix} 比较\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$



我们需要一种方法来比较这两组分布概率之间的距离。这就需要交叉熵损失



<font color="red">熵</font>（信息量的体现）： $H(p) = - \sum_x p(x)logp(x)$

<font color="red">交叉熵</f：$H(p,q) = - \sum_x p(x)logq(x)$， 表达p和q两个分布之间的关系

<font color="red">相对熵</font>：$KL(p||q) = - \sum_x p(x)log \frac {q(x)}{p(x)}$

​	相对熵（relative entropy）也叫KL散度（KL divergence）;用来度量两个分布之间的不相似性（dissimilarity）

​	注意，通常：$KL(p||q) \neq KL(q||p)$, 不具有交换性，因此KL散度不能叫距离，只能叫不相似性




三者之间的关系推导

$KL(p,q) = - \sum_x p(x)logq(x) = - (\sum_x p(x)logp(x) + \sum_x p(x) log \frac {q(x)}{p(x)}) = H(p) + KL(p||q)$

​	所以，交叉熵 = 熵 + 相对熵



由于我们的目标概率形式是

$\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$

在这种情况下，

$KL(p,q) = H(p) + KL(p||q) = KL(p||q)$

<font color="red">因此，我们直接使用交叉熵来度量来衡量两个随机分布差异即可</font>



<font color="red">$H(p,q) = - \sum_x p(x)logq(x)$</font>



对于例子中

$\begin{bmatrix} 0.21 \\ 0.01 \\ 0.78 \end{bmatrix} 比较\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$

随机分布差异 = $H(p,q) = - \sum_{i}^c p(x+i)logq(x_i) = -log(0.21) = 0.92$



##### 交叉熵损失 vs 多累支撑向量机损失

对于分类器结果 $\begin{bmatrix}0.6 \\ -2.3 \\ 1.9 \end{bmatrix}$, 当前分类鸟类，概率0.6



多类支撑向量机损失

$Li = \sum_{j \neq y_i max(0, S_j - S_{y_i} + 1)} = max(0, -2.3-0.6+1.0) + max(0, 1.9 - 0.6 + 1) = 2.3$



交叉熵损失

| $e^{f_i}$                                           | $\frac {t_i}{\sum_k t_j }$                           |
| --------------------------------------------------- | ---------------------------------------------------- |
| $\begin{bmatrix} 1.81 \\ 0.1 \\ 6.69 \end{bmatrix}$ | $\begin{bmatrix} 0.21 \\ 0.01 \\ 0.78 \end{bmatrix}$ |

$Li = -log(\frac{e^{S_{yi}}}{\sum_j e^{S_j}}) = -log(0.21) = 0.92$

![alt](images\对比交叉熵损失和多累支撑向量机损失.png)



我们再看一个例子，假设$y_i$第一列是当前真实分类的分数

| 假设分数f $y_i = 0$    | 交叉熵损失$Li = -log(\frac{e^{S_{yi}}}{\sum_j e^{S_j}})$ | 多类支撑向量机损失$Li = \sum_{j \neq y_i max(0, S_j - S_{y_i} + 1)}$ |
| ---------------------- | -------------------------------------------------------- | ------------------------------------------------------------ |
| [10(真实), -2, 3]      | 0.0004                                                   | 0                                                            |
| [10(真实), 9, 9]       | 0.2395                                                   | 0                                                            |
| [10(真实), -100, -100] | $1.5 * 10^{-48}$                                         | 0                                                            |



------------------------------------



#### 优化算法



##### 基于计算图的优化算法

1. 什么是计算图

   计算图是一种有向图，它用来表达输入，输出以及中间变量之间的计算关系，图中的每个节点对应着一种数学运算。

   

   可以用来推导任意复杂函数的求导问题。

   

   梯度下降算法要求损失函数对每一个$W_{ij}$的导数	$\frac{\partial L}{\partial W_{ij}}$

   比如$f(x,y) = (x+y)^2$

   ```mermaid
   graph LR
   	A[x] --> C
   	B[y] --> C
   	C((+)) -- z =x + y --> D((^2))
   	D --> E(f=z^2)
   
   ```

   ![alt](images\计算图例子1.png)

   

   ![alt](images\计算图梯度.png)

2. 反向传播例子

   - [ ] 重新看https://www.youtube.com/watch?v=FOt8n4wlPwc&list=PLFI1Cd4723_RQ6tTu-c2ZFFrMxtSIhztC&index=9  时间位置：1:04:05

     

     函数 $f(w,x) = \frac{1}{1 + e^{-(w_0x_0 + w_1x_1 + w_2)}}$

     

     $W = [w0, w1, w2, w3]$

     $w_0 = 3, w1 = -1, w2=-2, w3=1$

     $x_1 = -2$

     

     正向计算，我们要计算$f(w,x)$的输出

     ![alt](images\计算图例子3.png)

     

     正向计算没难度，那反向计算呢？这张图我们如何反向计算呢？

     通过反向逆运算，我们可以得到每一个点的相对梯度

     

     <font color="red">怎么算的？？？讲真我没看懂</font>

     ![alt](images\计算图例子4.png)

   

   计算图总结

   - 任意复杂的函数，都可以用计算图的形式来表示
   - 整个计算图中，每一个们单元都会得到一些输入，然后，进行下面两个计算
     - 这个门的输出值
     - 其输出至关于输入值的局部梯度
   - 利用链式法则，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度
   - 

   ```
   .md-inline-math {
       display: inline-block;
       font-size: 20px;
       line-height: 20px;
   }
   ```



##### 计算图的颗粒度



##### 计算图中常见的们单元

加法门：	a + b -> c

乘法门:      a * b -> c

拷贝门:

max门



----------------------------



#### 再谈激活函数
### 机器视觉 Chapter 3.1 损失函数和梯度计算和数据预处理

https://www.youtube.com/watch?v=AAfMEjR0mRY&list=PLFI1Cd4723_RQ6tTu-c2ZFFrMxtSIhztC&index=5



#### 本章内容

- 数据集	CIFAR10数据集，5000张样本，10000张测试样本，10个类别，尺寸32*32

- 分类器设计

  - 图像表示1

    - 基于像素的图像表示

  - 分类模型3

    - 线性分类器的定义
    - 线性分类器的权值
    - 线性分类器的分界面

  - 损失函数3

    - [x] 损失函数的定义
    - [x] 多类支持向量机损失
    - [x] 正则项与超参数

  - 优化算法2

    - [x] 什么是优化
    - [x] 梯度下降算法，随机梯度下降算法以及小批量梯度下降算法

  - 训练过程2

    - [x] 数据集划分

    - [x] 数据预处理

      

-------------------------



#### 损失函数

- 定义
- 多类支撑向量机损失
- 正则项与超参数



#### 损失函数定义

我们先看两个分类器来处理一张猫图

![alt](G:\文档\learning\计算视觉课程\images\猫.png)

我们假设猫图的表示向量是: $[56, 231, 24, 2]$

- 分类器1，分类正确

  $$ \begin{bmatrix} 0.2 & -0.5 & 0.1 & 2.0 \\ 1.5 & 1.3 & 2.1 & 0.0 \\ 0 & 0.25 & 0.25 & -0.3 \\ \end{bmatrix} x +  \begin{bmatrix} 1.1 \\ 3.2 \\ -1.2 \end{bmatrix} = f_i$$

|                                      | W1   |      |      |      | 图像（猫） |      | 偏移$b_i$ |      | 得分$f_i$                        |
| ------------------------------------ | ---- | ---- | ---- | ---- | ---------- | ---- | --------- | ---- | -------------------------------- |
| 汽车$w_1^T$                          | 0.2  | -0.5 | 0.1  | 2.0  | 56         |      | 1.1       |      | -97.9                            |
| <font color="green">猫$w_2^T$</font> | 1.5  | 1.3  | 2.1  | 0.0  | 231        | +    | 3.2       | =    | <font color="green">434.7</font> |
| 鸟$w_3^T$                            | 0    | 0.25 | 0.25 | -0.3 | 24         |      | -1.2      |      | 63.15                            |
|                                      |      |      |      |      | 2          |      |           |      |                                  |

- 分类器2，注意W2 ！= W1, b1 != b2

  $$ \begin{bmatrix} 0.2 & -0.1 & 0.1 & 1.7 \\ 1.5 & 0.2 & 2.1 & 5.0 \\ -1.2 & 1.5 & 0.25 & 0.5 \\ \end{bmatrix} x +  \begin{bmatrix} 2.1 \\ -0.2 \\ 2.4 \end{bmatrix} = f_i$$

  - <font color="red">预测结果错误</font>

|                                    | W2   |      |      |      | 图像表示x |      | 偏移$b_i$ |      | 得分$f_i$                      |
| ---------------------------------- | ---- | ---- | ---- | ---- | --------- | ---- | --------- | ---- | ------------------------------ |
| 汽车$w_1^T$                        | 0.2  | -0.1 | 0.1  | 1.7  | 56        |      | 2.1       |      | -6.1                           |
| 猫$w_2^T$                          | 1.5  | 0.2  | 2.1  | 5.0  | 231       | +    | -0.2      | =    | 190.6                          |
| <font color="red">鸟$w_3^T$</font> | -1.2 | 1.5  | 0.25 | 0.5  | 24        |      | 2.4       |      | <font color="red">286.3</font> |
|                                    |      |      |      |      | 2         |      |           |      |                                |

矩阵$w_i$的行数由类别数量决定，列数和图片向量长度相同



在这里，很明显，这里分类器1要优于分类器2。



<font color="red">我们如何进行定量分析呢？机器怎么做，机器如何知道哪个分类器更好？</font>



我们希望损失函数可以和参数有某种关联。

- 给某一个参数的时候，损失函数会给一个值。
- 函数参数和函数之间是对应的，不需要分阶段来。不是说你给我一个参数，我计算出他的值，再去和真是结果比对



上面的例子是：

给出一个分类器的权值矩阵W，我们代入图片向量x进行计算，取最大结果判断图片类别，在和真实结果比对。这时候答案比较和权值过程已经没关系了，变成了两阶段过程。



<b>我们希望是，有一个函数，输入一组w和b，就能给出一组输出。这就是损失函数。</b>



损失函数关键

- <font color="red">`损失函数`是一个函数，用于度量给定分类器的预测值与真实值的不一会程度，其输出通常是一个非负实数值。</font>

- <font color="red">损失函数返回的非负实值可以作为反馈信号来对分类器参数进行调整，以降低当前实例对应的损失值，提升分类器的分类效果</font>

我们需要在参数和损失值之间建立一个函数关系。



<font color="red">损失函数的一般性数学定义</font>

$L = 1/n \sum L_i(f(x_i, W), y_i)$

- x表示图像数据
- W表示分类器对$x_i$的类别预测，包含b
- $y_i$为样本i真实类别标签（整数）
- $L_i$表示第i个样本的损失当前预测值
- L为数据集损失，他是数据集中所有样本的损失的平均



#### 多类支撑向量机的损失

$L = \frac{1}{n} \sum L_i(f(x_i, W), y_i)$

我们关注计算单个样本i的损失

$L_i(f(x_i, W), y_i)$， 让w和损失挂钩



分类器

$S_{ij} = f_i(x_i, w_j, b_j) = w_j^Tx_i + b_j$				

j：类别标签，取值范围{1,2, ........ , c}

$w_j$, $b_j$：第j个类别的分类器参数，特征向量 

$x_i$：数据集中第i个样本

$s_{ij}$：第i个样本第j类别的预测分数



##### 多类支撑向量机损失定义如下

$$L_i = \sum_{j!=y_i} \begin{cases} 0 & s_{y_i} >= s_{ij} + 1 \\ s_{ij} - s_{y_i} +1 & otherwise \end{cases} = \sum_{j!=y_i} max(0, s_{ij} - s_{y_i} +1) $$

$y_i$：第i个样本的真实类别

$s_{ij}$： 第i个样本第j类别的预测分数

$s_{y_i}$：当第i个样本真实类别的预测分数

- 当第i个样本真实类别的预测分数$s_{y_i}$  大于 第i个样本第j个类别的预测分数$s_{ij} + 1$ （+1是为了明显区分，增加边界防止噪声） ，注意条件中j != $y_i$。此时，说明预测结果中，真实类别分支远大于其他分类预测分支（至少1），所以此时函数的损失是0，结果非常好。
  - 简单说，正确分类计算结果比其他不争取的高出1分，就没有损失

- 否则，$s_{y_i} >= s_{ij} + 1$不成立，则$s_{y_i} < s_{ij} + 1$，即使 $s_{ij} - s_{y_i} + 1 > 0$, 大的越多，损失越大，因此我们用它来衡量

- 因此
  - $$L_i = \sum_{j!=y_i} max(0, s_{ij} - s_{y_i} +1) $$



##### 计算例子

假设有三个类别的训练样本各一张，分类器是线性分类器$f(x, W) = Wx + b$，其中权值W，b已知，分类器对三个样本的打分如下：

|      | 鸟                           | 猫                           | 车                           | 损失值                       |
| ---- | ---------------------------- | ---------------------------- | ---------------------------- | ---------------------------- |
| 1鸟  | <font color="red">0.6</font> | -2.3                         | 1.9                          | <font color="red">2.3</font> |
| 2猫  | 1.7                          | <font color="red">2.9</font> | 2.3                          | <font color="red">0.4</font> |
| 3车  | 3.1                          | -2.6                         | <font color="red">4.3</font> | <font color="red">0</font>   |

- 对于1鸟，正确的分类是鸟

  <font color="red">$s_{ij}$： 第i个样本第j类别的预测分数</font>

  $$L_i = \sum_{j!=y_i} max(0, s_{ij} - s_{y_i} +1) $$ = max(0, -2.3 - 0.6 + 1) + max(0, 1.9 - 0.6 + 1) = max(0, -1.9) + max(0, 2.3) = 2.3

- 对于2猫，争取类别是猫

  $$L_i = \sum_{j!=y_i} max(0, s_{ij} - s_{y_i} +1) $$ = max(0, 1.7 - 2.9 + 1) + max(0, 2.3 - 2.9 + 1) = max(0, -0.2) + max(0, 0.4) = 0.4

- 对于3车

  $$L_i = \sum_{j!=y_i} max(0, s_{ij} - s_{y_i} +1) $$ = max(0, 3.1 - 4.3 + 1) + max(0, -2.6 - 4.3 + 1) = max(0, -0.2) + max(0, -5.9) = 0

因此，当前这个分类器的总算是就是

$L = \frac{1}{n} \sum L_i(f(x_i, W), y_i) = \frac{1}{3} (2.3 + 0.4 + 0) = 2.7/3 = 0.9$ 



本段内容重点

1. 损失函数

   $L = 1/n \sum L_i(f(x_i, W), y_i)$

2. 单样本的多类支撑向量机损失

   $$L_i = \sum_{j!=y_i} max(0, s_{ij} - s_{y_i} +1) $$

3. 线性分类器

   $S_{ij} = f_i(x_i, w_j, b_j) = w_j^Tx_i + b_j$	



问题抢答

1. 多类支撑向量机损失$L_i$的最大/最小值是多少？

   很明显，最小值是0。最大值则很有可能会是无穷。假设我给了一张猫图，但狗的分数无穷大

2. 如果初始化是w和b很小，损失L会是多少？

   假设w和b都是0，则$s_{ij} = 0$, $s_{ij} = 0$, $s_{y_i} = 0$，因此我们

   $$L_i = \sum_{j!=y_i} max(0, s_{ij} - s_{y_i} +1) = \sum_{j!=y_i} 1 = 类别数 - 1$$

3. 考虑所有类别的包括$j=y_i$，损失$L_i$会有什么变化？

   对于$j=y_i$，$s_{ij} - s_{y_i} + 1 = 1$

   多出来一项自己和自己相减，sum多1。对总损失没什么用，但是没什么意义

4. 总损失L计算时，如果用求和代替平均？

   不要$L = 1/n \sum L_i(f(x_i, W), y_i)$中的$1/n$，则总损失值L变大n倍，不影响比较取出最小的损失值，蛋没有意义

5. 如果使用$$L_i = \sum_{j!=y_i} max(0, s_{ij} - s_{y_i} +1)^2 $$

   有影响。损失被放大，或者缩小。比如我们max=0.1, 平方之后变成0.01，还是会有影响的



#### 再谈损失函数

损失函数

$L = 1/n \sum L_i(f(x_i, W), y_i)$



问题：假设存在一个W使得损失函数L=0，这个W是唯一的么



假设有两个分类器$f_1(x, W_1) = W_1x$，$f_2(x, W_2) = W_2x$，其中$W_2=2W_1$

对于下面图像，已知分类器1的打分结果，请计算分类器2的打分结果

![alt](images\汽车.png)

|         | 鸟                           | 猫                            | <font color="red">汽车</font> | 损失 |
| ------- | ---------------------------- | ----------------------------- | ----------------------------- | ---- |
| 分类器1 | 3.1                          | -2.6                          | 4.3                           | 0    |
| 分类器2 | <font color="red">6.2</font> | <font color="red">-5.2</font> | <font color="red">8.6</font>  | 0    |

第i个样本的第j个项的打分：$S_{ij} = f_i(x_i, w_j, b_j)$

因此 $f_2(x, W_2) = W_2x = 2$f_1(x, W_1)$, 全部打分翻倍



所以，使得损失函数L=0的W不唯一



那么W1和W2我们如何选择呢？

我们需要引入<font color="red">正则项</font>



#### 正则项与超参数

##### 带正则项的损失函数数学定义

$L(W) = \frac{1}{N} \sum_{i} L_i(f(x_i, W), y_i) + \lambda R(W)$

数据损失：$\frac{1}{N} \sum_{i} L_i(f(x_i, W), y_i)$

正则项： $\lambda R(W)$， 防止模型在训练数据上学的"太好"，出现过拟合。

​	$R(W)$： 只和权值W有关，与图像数据无关

​	$\lambda$：<font color="red">超参数</font>，控制着正则损失在总损失中的比重。学习之前设置的参数，而不是学习得到的



##### 超参数

超参数控制着正则损失在总损失中的比重，在学习之前设置的参数。会对模型性能有重要影响。

- 如果$\lambda = 0$， 则退化为不带正则项的损失函数
- 如果$\lambda 趋于 \infin$，则模型学习的值起不到作用



如何设置一个好的参数，我们后面课程中讨论。



##### L2正则项

数学定义

$R(W) = \sum_{k} \sum_{i} W_{k,l}^2$



样本 $x = [1, 1, 1, 1]$

分类器1：$W_1 = [1, 0, 0, 0]$

分类器1：$W_2 = [0.25, 0.25, 0.25, 0.25]$

分类器输出: $W_1^Tx = $W_2^Tx = 1$

正则损失

​	$R(W_1) = 1^2 + 0 ^2 + 0^2 + 0^2 = 1$

​	$R(W_2) = 0.25^2 + 0.25^2 + 0.25^2 + 0.25^2 + = 0.25$， 权值分散，容错性能更好一些

原始损失一样的情况下，L2正则项会让总损失不一样，我们取小的。



正则项的实际用途用途

- 让解唯一
- 让解每个维度的特征都被应用进来，让模型有了偏好
- 防止模型学习的"过好"(过拟合)



##### L1正则项

$R(W) = \sum_{k} \sum_{i} |W_{k,l}|$



##### Elastic net(L1 + L2)

$R(W) = \sum_{k} \sum_{i} \beta W_{k,l}^2 + |W_{k,l}|$



--------------------------------



#### 优化算法

损失值和真实值差异很大的时候，我们怎么去让他变得更好呢？这时候就需要优化算法



##### 什么是优化

参数优化是机器学习的核心步骤之一，它利用损失函数的输出值作为反馈信号来调整分类器参数，以<font color="red">提升</font>分类器对<font color="red">训练样本</font>的<font color="red">预测性能</font>



##### 优化算法的目标

让损失函数L是一个与参数W有关的函数，优化的目标就是找到使损失函数L达到最优的那组参数W。

损失函数

​	$L = \frac{1}{N} \sum_{i=1}^N L_i + \lambda R(W)$

直接方法

​	$\frac{\partial L}{\partial W} = 0$，L对W偏导等于0。注意此处是导数=0，绝大多数模型L是不可能为0的。我们需要的找出最小的L



我们需要解下面的方程

对于矩阵$W = \begin{bmatrix} w11 & ......\\ w21 & ...... \\ .&   ...... \\ w101 & ......\end{bmatrix}$, 我们认为W有10个向量构成$w_1,w_2....,w_10$，则我们需要求解方程

$$\begin{cases} \frac{\partial L}{\partial W_1} = 0 \\ \frac{\partial L}{\partial W_2} = 0 \\ ....... \\ \frac{\partial L}{\partial W_10} = 0 \end{cases} $$



我们很难从这个方程直接求解出矩阵W。需要优化算法来支持，数学里的优化问题



#### 梯度下降算法

一种简单而高效的优化算法



一维的情况

1. 往哪走？

   ![alt](images\梯度下降算法.png)

   这个图像比较简单我们可以直接看到最优点在哪里，但是通常实际函数都非常复杂，我们只能知道他的局部特性，并不能知道整体最优解在哪里。

   

   答：负梯度方法

   

2. 走多远

   答：步长来决定，学习率

   

三维的情况

![alt](images\三维梯度下降.png)



梯度下降算法：利用所有样本计算损失并更新梯度

```python
while True:
	权值的梯度 <- 计算梯度(损失, 训练样本, 权值)
    权值 <- 权值 - 学习率 * 权值的梯度
    
    # 结束条件
    权值差异足够小，或者达到谷底
```



#### 如何计算梯度

##### 数值法：验证自己解析法的结果是否正确

<font color="red">计算量大不精确，容易写</font>

一维变量，函数求导

$\frac{dL(w)}{dw} = \lim_{h-0} \frac{L(w+h) - L(w)}{h}$



例子，比如我们要求$L(W)=W^2$，求W=1点的梯度

我们取一个足够小的h=0.0001，带入计算即可

$\frac{dL(w)}{dw} = \lim_{h-0} \frac{L(w+h) - L(w)}{h} \approx \frac{L(1+0.0001) - L(1)}{0.0001} = \frac{1.0001^2 - 1^2}{0.0001} = 2.0001$

这里计算的结果是近似结果



##### 解析法：真实场景使用

<font color="red">精确，速度快，易出错</font>

牛顿-莱布尼茨

![alt](images\牛顿-莱布尼茨.png)

莱布尼茨的学生师承。。。真可怕

莱布尼茨 --> 伯努利 --> 欧拉 --> 拉格朗日 -> 柯西 -> 高斯 -> 黎曼



我们只要手动解出导数公式即可



例子，比如我们要求$L(W)=W^2$，求W=1点的梯度

$\nabla L(w) = 2w$

$\nabla_{w=1} L(w) = 2$



手动推到不太容易，数学功底要求有点高



<font color="red">求梯度时，我们一般使用解析梯度，而数值梯度主要用于解析梯度的正确性校验</font>



##### 作业：梯度计算

TODO ：重新看这段计算过程

https://www.youtube.com/watch?v=AAfMEjR0mRY&list=PLFI1Cd4723_RQ6tTu-c2ZFFrMxtSIhztC&index=5

时间位置1:09:57



如何计算多类支撑向量机损失的导数函数

$\begin{cases} L_i = \sum_{j \neq y_i} max(0, S_{ij} - S_{y_i} + 1) \\ S_{ij} = W_j^TX_i + b_j \end{cases}$

so, $L_i = \sum_{j \neq y_i} max(0, W_j^TX_i + b_j - (W_{y_i}^TX_i + b_{y_i}) + 1)$



求解过程

$y = max(0, w^2-1)$

$\frac{\partial y}{\partial W} = \begin{cases} 2w & w^2-1 >= 0 \\ 0 & w^2-1 < 0 \end{cases} $$



##### 梯度下降的计算效率

当N很大时，权值的梯度计算量很大



##### 改进： 随机梯度下降算法

速度更快

![alt](images\随机梯度下降.png)

每次从数据集中随机取出一个样本，，用于计算权值的梯度



##### 改进：小批量梯度下降算法

每次随机选一个批量m个昂本，计算损失并更新梯度，m通常取2的倍数

![alt](images\小批量梯度下降.png)

关键值

- 样本总量N

- 迭代次数：每选m个样本，就更新一次，叫迭代
- batch-size：一次迭代使用的样本数，就是上面的m
- epoch：表示过了1遍训练集中的所有样本。一个epoch包含多少迭代呢？N/m



##### 梯度下降总结

![alt](images\梯度下降总结.png)

-------------------

#### 训练过程

##### 数据集划分

数据集：所有带标签的数据集



1. 数据集划分为2粉

- 训练集：训练模型，寻找最优分类器
- 测试集：评估模型，评测泛化能力

![alt](images\数据划分1.png)

如果模型含有超参数，比如正则化强度，如何找到泛化能力最好的超参数？

训练集调超参数，测试集选超参数？

<font color="red">这样不行</font>



<font color="red">真实的数据划分</font>

2. 数据集划分为3粉

- 训练集：训练模型，寻找最优分类器
- 验证集：用于选择超参数。模型给出之前，当做不可见
- 测试集：评估模型，评测泛化能力

![alt](images\数据划分2.png)



数据集很少怎么办？



3. k折交叉验证

   问题：如果数据很少，那么可能验证集包含的样本就太少，从而无法在统计上代表数据

   解决：把数据集分为k份

   ![alt](images\k折交叉验证.png)

   

#### 数据预处理



1. 原始数据 -> 去均值 -> 归一化，神经网络常用

![alt](images\数据预处理1.png)

2. 原始数据 -> 去相关 -> 白化，支撑向量机常用

   ![alt](images\数据预处理2.png)

